{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Autor_**: Rubén del Mazo Rodríguez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de una red neuronal profunda paso a paso\n",
    "\n",
    "Anteriormente se entrenó una red neuronal con una única capa oculta. En este archivo se quita la última restricción, el número de capas, y se implementan las funciones necesarias para construir una red neuronal profunda con tantas capas y unidades ocultas como se desee. \n",
    "\n",
    "Se volverá a crear la red neuronal con una única capa oculta, pero cambiando su estructura y funciones de activación, adaptándolas al problema de clasificación para el que será utilizada. Y las funciones de esta nueva estructura serán aprovechadas para crear el caso general de $L$ capas ocultas. De esta manera, podremos comparar en el siguiente y último archivo la diferencia de rendimiento entre una red con una capa oculta y una red con $L-1$ capas ocultas.\n",
    "\n",
    "**Puntos principales:**\n",
    "\n",
    "- Construir las funciones necesarias para una red neuronal profunda con L capas. \n",
    "- Utilizar unidades ocultas con la función de activación que faltaba por mostrar, ReLU, la cual da mejores resultados en general.\n",
    "- Implementar la estructura de la red en una clase fácil de utilizar.\n",
    "\n",
    "**Notación**:\n",
    "- El superíndice $[l]$ indica una cantidad o variable asociada con la capa l-ésima. \n",
    "    - Por ejemplo: $a^{[L]}$ es la matriz de activación/salida de la capa L. $W^{[L]}$ y $b^{[L]}$ son los parámetros de la capa L, siendo el primero la matriz de pesos y el segundo el vector de sesgos.\n",
    "- El superíndice $(i)$ indica una cantidad o variable asociada al ejemplo de entrenamiento i-ésimo.\n",
    "    - Ejemplo: $x^{(i)}$ es el ejemplo de entrenamiento i-ésimo.\n",
    "- El subíndice $k$ indica la fila k-ésima de un vector o matriz, mientras que el subíndice $j$ señala la columna j-ésima. También pueden indicar el número de unidad oculta o \"neurona\" en una capa l-ésima.\n",
    "    - Ejemplo: $a^{[l](i)}_k$ indica el valor de activación correspondiente a la fila (neurona) k-ésima la capa l-ésima del vector de entrenamiento i-ésimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenidos\n",
    "- [1 - Librerías](#1)\n",
    "- [2 - Esquema general del ciclo de una red neuronal profunda](#2)\n",
    "- [3 - Modelo de red neuronal de dos capas (una capa oculta)](#3)\n",
    "    - [3.1 - Inicialización de los parámetros](#3-1)\n",
    "    - [3.2 - Propagación hacia delante (_forward propagation_)](#3-2)\n",
    "        - [3.2.1 - Funciones de activación](#3-2-1)\n",
    "        - [3.2.2 - Cálculo de la función lineal de propagación](#3-2-2)\n",
    "        - [3.2.3 - Cálculo de las funciones de activación](#3-2-3)\n",
    "    - [3.3 - Cálculo del coste global](#3-3)\n",
    "    - [3.4 - Retropropagación (_backward propagation_)](#3-4)\n",
    "        - [3.4.1 - Derivadas de las funciones de activación](#3-4-1)\n",
    "        - [3.4.2 - Cálculo de las derivadas de la función lineal](#3-4-2)\n",
    "        - [3.4.3 - Cálculo de las derivadas de las funciones de activación](#3-4-2)\n",
    "    - [3.5 - Actualización de los parámetros](#3-5)\n",
    "- [4 - Modelo de red neuronal con L capas (L-1 capas ocultas)](#4)\n",
    "    - [4.1 - Inicialización de los parámetros](#4-1)\n",
    "    - [4.2 - Propagación hacia delante (_forward propagation_)](#4-2)\n",
    "    - [4.3 - Cálculo del coste global](#4-3)\n",
    "    - [4.4 - Retropropagación (_backward propagation_)](#4-4)\n",
    "    - [4.5 - Actualización de los parámetros](#4-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Librerías\n",
    "\n",
    "En primer lugar, importamos y ejecutamos las librerías necesarias:\n",
    "\n",
    "- [numpy](https://numpy.org/doc/1.24/) es el paquete fundamental para la computación científica con Python.\n",
    "- `np.random.seed(1)` se utiliza para mantener la coherencia de todas las llamadas a funciones aleatorias. Ya se ha hablado sobre ello en archivos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Esquema general del ciclo de una red neuronal profunda\n",
    "\n",
    "Como hasta ahora se ha explicado y mostrado en los archivos anteriores, la construcción de una red neuronal consta de varios pasos y se crearán funciones que cumplan cada uno de ellos. El esquema para su creación es el siguiente:\n",
    "\n",
    "- Inicialización de los parámetros (tanto para el caso de la red neuronal de dos capas como la de $L$ capas).\n",
    "- Implementar el módulo de propagación hacia delante (en morado en la figura inferior):\n",
    "     - Completar la parte LINEAL del paso de propagación hacia delante de la capa, siendo el resultado $Z^{[l]}$.\n",
    "     - Definir/disponer de la función de ACTIVACIÓN, en este caso, ReLU y sigmoide (`relu()` y `sigmoide()`)\n",
    "     - Combinar los dos pasos anteriores en la función de activación hacia delante, [LINEAL -> ACTIVACIÓN]\n",
    "         - En esta red particular, realizaremos la combinación [LINEAL -> RELU] L-1 veces (para las capas 1 a la L-1) y se añade [LINEAL -> SIGMOIDE] al final para la última capa $L$. Con esto tendríamos la propagación hacia delante para un modelo de L capas.\n",
    "- Calcular la pérdida/coste.\n",
    "- Implementar el módulo de la retropropagación (en rojo en la figura inferior):\n",
    "    - Completar la parte LINEAL del paso de retropropagación de la capa.\n",
    "    - Definir/disponer de las funciones que implementan el GRADIENTE de las funciones de ACTIVACIÓN (`relu_backward()` y `sigmoide_backward()`). \n",
    "    - Combinar los dos pasos anteriores en la función de retropropagación, [LINEAL -> ACTIVACION].\n",
    "        - En esta red particular, realizaremos la combinación [LINEAL -> RELU_backward] L-1 veces (para las capas 1 a la L-1) y se añade [LINEAL -> SIGMOIDE_backward] al final para la última capa $L$. Con esto tendríamos la retropropagación para un modelo de L capas.\n",
    "- Finalmente, se actualizan los parámetros.\n",
    "\n",
    "La siguiente imagen muestra esta descripción:\n",
    "\n",
    "<img src=\"imagenes/esquema_general.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center><b>Figura 1.</b> Esquema general del modelo de red neuronal profunda.</center></caption><br>\n",
    "\n",
    "\n",
    "**Notas de implementación**:\n",
    "\n",
    "Para cada función de propagación hacia delante, existe una función de retropropagación. Por eso, en cada paso del módulo de propagación hacia delante se almacenarán algunos valores en una \"cache\". Estos valores almacenados en caché son útiles para calcular gradientes. En el módulo de retropropagación, se puede utilizar la caché para calcular los gradientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Modelo de red neuronal de dos capas (una capa oculta)\n",
    "\n",
    "<a name='3-1'></a>\n",
    "### 3.1 - Inicialización de los parámetros\n",
    "\n",
    "En este apartado se crea la función que inicializa los parámetros para la red neuronal de 2 capas (una capa oculta).\n",
    "\n",
    "- La estructura del modelo es: *LINEAL -> RELU -> LINEAL -> SIGMOIDE*. \n",
    "- Se utilizará una inicialización aleatoria de la matriz de pesos: `np.random.randn() * 0.01`. Documentación para [np.random.randn](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html).\n",
    "- Se utilizará una inicialización con ceros para los sesgos: `np.zeros()`. La documentación para [np.zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html).\n",
    "\n",
    "> Nota: Es igual que en la red neuronal de una capa oculta implementada en el anterior archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c468c89deb6d0cacf2ade5ab4151d26e",
     "grade": false,
     "grade_id": "cell-96d4e144d9419b32",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def inicializar_parametros(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    n_x -- tamaño de la capa de entrada\n",
    "    n_h -- tamaño de la capa oculta\n",
    "    n_y -- tamaño de la capa de salida\n",
    "    \n",
    "    Devuelve:\n",
    "    parametros -- diccionario python que contiene los parámetros:\n",
    "                    W1 -- matriz de pesos de la forma (n_h, n_x)\n",
    "                    b1 -- vector de sesgos de la forma (n_h, 1)\n",
    "                    W2 -- matriz de pesos de la forma (n_y, n_h)\n",
    "                    b2 -- vector de sesgos de la forma (n_y, 1)                   \n",
    "    \"\"\"   \n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros(shape=(n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros(shape=(n_y, 1))    \n",
    "\n",
    "    # Asegurar que las dimensiones son las correctas\n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parametros = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Propagación hacia delante (_forward propagation_)\n",
    "\n",
    "Una vez inicializados los parámetros, continuamos con el módulo de propagación hacia delante. Para la red neuronal de dos capas se necesitan dos funciones, las cuales se utilizarán luego en el modelo general de L capas:\n",
    "\n",
    "- LINEAL\n",
    "- LINEAL -> ACTIVACION donde ACTIVACION es, en esta red, ReLU o sigmoide.\n",
    "\n",
    "Definamos en primer lugar las funciones de activación:\n",
    "\n",
    "<a name='3-2-1'></a>\n",
    "#### 3.2.1 - Funciones de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(Z):\n",
    "    \"\"\"\n",
    "    Calcula el sigmoide de Z\n",
    "    \n",
    "    Argumentos:\n",
    "    Z -- array de NumPy de cualquier dimension (salida lineal de la capa)\n",
    "    \n",
    "    Devuelve:\n",
    "    A -- resultado de sigmoide(Z), de las mismas dimensiones que Z.\n",
    "    cache -- devuelve el argumento de entrada, Z, que se utilizara en la retropropagacion.\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implementa la funcion ReLU.\n",
    "\n",
    "    Argumentos:\n",
    "    Z -- array de NumPy de cualquier dimension (salida lineal de la capa)\n",
    "\n",
    "    Returns:\n",
    "    A -- resultado de aplicar relu(Z), de las mismas dimensiones que Z. Es el parametro post-activacion.\n",
    "    cache -- devuelve el argumento de entrada, Z, que se utilizara en la retropropagacion.\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2-2'></a>\n",
    "#### 3.2.2 - Cálculo de la función lineal de propagación\n",
    "\n",
    "A continuación se crea la parte lineal de la propagación hacia delante la cual, vectorizada para todos los ejemplos de entrenamiento, sigue la siguiente ecuación.\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{1}$$\n",
    "\n",
    "donde $A^{[0]} = X$ y $X$ es la matriz de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "770763ab229ee87e8f5dfd520428caa3",
     "grade": false,
     "grade_id": "cell-4d6e09486a53f4c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_lineal(A, W, b):\n",
    "    \"\"\"\n",
    "    Implementa la parte lineal de la propagación hacia delante de una capa.\n",
    "\n",
    "    Argumentos:\n",
    "    A -- activaciones de la capa anterior (o datos de entrada): (tamaño de la capa anterior, numero de ejemplos).\n",
    "    W -- matriz de pesos: matriz numpy de dimensiones (tamaño de la capa actual, tamaño de la capa anterior).\n",
    "    b -- vector de sesgos: matriz numpy de dimensiones (tamaño de la capa actual, 1).\n",
    "\n",
    "    Devuelve:\n",
    "    Z -- la entrada de la función de activación, tambien llamado parametro de pre-activacion .\n",
    "    cache -- una tupla python que contiene \"A\", \"W\" y \"b\" ; almacenada para calcular eficientemente la retropropagación.\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    # Aseguramos que las dimensiones son correctas\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2-3'></a>\n",
    "##### 3.2.3 - Cálculo de las funciones de activación\n",
    "\n",
    "En esta red neuronal se utilizan dos funciones de activación:\n",
    "\n",
    "- **Sigmoide**: Matemáticamente es $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. Se ha implementado en la función `sigmoide()`.\n",
    "\n",
    "- **ReLU**: Matemáticamente es $A = ReLU(Z) = max(0, Z)$. Se ha implementado en la función `relu()`.\n",
    "\n",
    "Ambas funciones devuelven dos variables: el valor de activación, \"`A`\" y un \"`cache`\" que contiene \"`Z`\" (aquello que se proveerá a la función de retropropagación correspondiente). Para usarlas, realizamos las siguientes llamadas:\n",
    "\n",
    "``` python\n",
    "A, activacion_cache = sigmoide(Z)\n",
    "\n",
    "A, activacion_cache = relu(Z)\n",
    "```\n",
    "\n",
    "Por comodidad, se agruparán las funciones que calculan el paso lineal y el de activación en una sola función que se definirá a continuación. Su relación matemática para cualquier capa, $l$, en todo el conjunto de entrenamiento es:\n",
    "\n",
    "$$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})\\tag{2}$$\n",
    "\n",
    "donde la función de activación \"g\" puede ser `sigmoide()` o `relu()`. Utilizaremos `forward_lineal()` y la correspondiente función de activación.\n",
    "\n",
    ">**Note**: En el aprendizaje profundo, el cálculo \"[LINEAL->ACTIVACION]\" cuenta como una única capa en una red neuronal, no como dos capas. Se podría definir todo en la misma función, pero se ha decidido modularizar cada apartado para mejor comprensión y explicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f09e76f2a56c8ee77db3e89214a676b2",
     "grade": false,
     "grade_id": "cell-eb48903dd8e48a90",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_activacion_lineal(A_prev, W, b, activacion):\n",
    "    \"\"\"\n",
    "    Implementar la propagación hacia delante para la capa de la forma LINEAL->ACTIVACIÓN\n",
    "\n",
    "    Argumentos:\n",
    "    A_prev -- activaciones de la capa anterior (o datos de entrada): (tamaño de la capa anterior, numero de ejemplos)\n",
    "    W -- matriz de pesos: matriz numpy de forma (tamaño de la capa actual, tamaño de la capa anterior)\n",
    "    b -- vector de sesgo, matriz numpy de forma (tamaño de la capa actual, 1)\n",
    "    activacion -- la funcion de activacion a utilizar en esta capa, almacenada como cadena de texto: \"sigmoide\" o \"relu\".\n",
    "\n",
    "    Devuelve:\n",
    "    A -- la salida de la funcion de activacion, tambien llamada valor post-activación \n",
    "    cache -- una tupla python que contiene \"cache_lineal\" y \"activacion_cache\";\n",
    "             almacenada para calcular la retropropagacion eficientemente\n",
    "    \"\"\"\n",
    "    \n",
    "    if activacion == \"sigmoide\":\n",
    "        Z, cache_lineal = forward_lineal(A_prev, W, b)\n",
    "        A, activacion_cache = sigmoide(Z)\n",
    "    elif activacion == \"relu\":\n",
    "        Z, cache_lineal = forward_lineal(A_prev, W, b)\n",
    "        A, activacion_cache = relu(Z)        \n",
    "    else:\n",
    "        print(\"¡Error! Solo se admiten relu o sigmoide como parametros en \\\"activacion\\\"\")\n",
    "    \n",
    "    # Aseguramos dimensiones\n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "\n",
    "    cache = (cache_lineal, activacion_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "### 3.3 - Cálculo del coste global\n",
    "\n",
    "Una vez calculada $A^{L}$ hay que calcular el coste para comprobar que el modelo está aprendiendo los parámetros óptimos. Como ya es habitual, se utilizará la pérdida de entropía cruzada para un problema de clasificación binario:\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{3}$$\n",
    "\n",
    "Para esta red neuronal, $L$ = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17919bb7d82635554b52aed7e96e8d9b",
     "grade": false,
     "grade_id": "cell-abad606772066f14",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calcular_coste(AL, Y):\n",
    "    \"\"\"\n",
    "    Calcula el coste de la entropía cruzada dado en la ecuación (3).\n",
    "\n",
    "    Argumentos:\n",
    "    AL -- La salida de la activación de la capa L, de dimensiones (1, numero de casos de entrenamiento).\n",
    "          Corresponde con el vector de probabilidades asociado a las etiquetas verdaderas, de las mismas dimensiones.\n",
    "    Y -- Vector de etiquetas verdaderas de dimensiones (1, número de casos de entrenamiento).\n",
    "\n",
    "    Devuelve:\n",
    "    coste -- coste de la entropia cruzada dada la ecuacion (3).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Numero de casos de entrenamiento\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Calculo de la perdida de entropia cruzada\n",
    "    # Forma 1\n",
    "    # perdida = np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL))\n",
    "    # coste = (-1 / m) * np.sum(perdida)\n",
    "\n",
    "    # Forma 2\n",
    "    coste = (1. / m) * (- np.dot(Y, np.log(AL).T) - np.dot(1 - Y, np.log(1 - AL).T))\n",
    "    coste = np.squeeze(coste) # Por ejemplo, convierte [[17]] en 17\n",
    "    \n",
    "    assert(coste.shape == ())\n",
    "\n",
    "    return coste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-4'></a>\n",
    "### 3.4 - Retropropagación (_backward propagation_)\n",
    "\n",
    "Tal y como se hizo para el módulo de propagación hacia delante, se implementarán funciones para la retropropagación. Se calculará el gradiente de la función de coste con respecto a los parámetros. El gráfico computacional es:\n",
    "\n",
    "<img src=\"imagenes/grafico_computacional.png\">\n",
    "<caption><center><b>Figura 2</b>: Propagación hacia delante junto con la retropropagación para la estructura de cálculo de activaciones [LINEAL -> RELU ($g^{[1]}$) -> LINEAL -> SIGMOIDE ($g^{[2]}$)]. Los bloques negros representan la propagación hacia delante, mientras que los bloques naranjas representa la retropropagación.</center></caption>\n",
    "\n",
    "La regla de la cadena del cálculo que se utiliza para derivar el coste con respecto a los parámetros es, por ejemplo, para $Z^{[1]}$, en el caso de la red de dos capas:\n",
    "\n",
    "$$dZ^{[1]} = \\frac{d \\mathcal{J}(A^{[2]},Y)}{{dZ^{[1]}}} = \\frac{d\\mathcal{J}(A^{[2]},Y)}{{dA^{[2]}}}\\frac{{dA^{[2]}}}{{dZ^{[2]}}}\\frac{{dZ^{[2]}}}{{dA^{[1]}}}\\frac{{dA^{[1]}}}{{dZ^{[1]}}} \\tag{4} $$\n",
    "\n",
    "Para calcular el gradiente de $dW^{[1]} = \\frac{\\partial J}{\\partial W^{[1]}}$, se utiliza la ecuación (4): $dW^{[1]} = dZ^{[1]} \\times \\frac{\\partial Z^{[1]} }{\\partial W^{[1]}}$. Durante la retropropagación, en cada paso se multiplica el gradiente de turno por el gradiente correspondiente a la capa específica para obtener el gradiente deseado.\n",
    "\n",
    "De forma equivalente, para calcular el gradiente $db^{[1]} = \\frac{\\partial J}{\\partial b^{[1]}}$, se utiliza: $db^{[1]} = dZ^{[1]} \\times \\frac{\\partial Z^{[1]} }{\\partial b^{[1]}}$. De ahí que se hable de **retropropagación (_backpropagation_)**.\n",
    "\n",
    "La construcción de la retropropagación es similar a la de la propagación hacia delante, donde implementaremos dos funciones:\n",
    "\n",
    "1. LINEAL backward\n",
    "2. LINEAL -> ACTIVACION backward donde ACTIVACION es, en esta red, la derivada de la función ReLU o sigmoide.\n",
    "\n",
    "Importante tener en cuenta que:\n",
    "\n",
    "- `b` es una matriz (np.ndarray) con una columna y $n^{[l]}_h$ filas. Por ejemplo: b = [[1.0], [2.0]].\n",
    "- `np.sum()` realiza la suma sobre todos los elementos del ndarray.\n",
    "- axis = 1 o axis = 0 especifica si la suma se realiza por filas o por columnas, respectivamente.\n",
    "- _keepdims_ especifica si las dimensiones originales de la matriz se mantienen o no. Es decir, por ejemplo, si la matriz era 2D, que se mantenga 2D y no pase a ser 1D.\n",
    "\n",
    "Veamos estos puntos con un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de A:  (2, 2) \n",
      "\n",
      "axis=1 y keepdims=True\n",
      "[[3]\n",
      " [7]]\n",
      "(2, 1) \n",
      "\n",
      "axis=1 y keepdims=False\n",
      "[3 7]\n",
      "(2,) \n",
      "\n",
      "axis=0 y keepdims=True\n",
      "[[4 6]]\n",
      "(1, 2) \n",
      "\n",
      "axis=0 y keepdims=False\n",
      "[4 6]\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "print(\"Dimensiones de A: \", A.shape, \"\\n\")\n",
    "print('axis=1 y keepdims=True')\n",
    "print(np.sum(A, axis=1, keepdims=True))\n",
    "print(np.sum(A, axis=1, keepdims=True).shape, \"\\n\")\n",
    "print('axis=1 y keepdims=False')\n",
    "print(np.sum(A, axis=1, keepdims=False))\n",
    "print(np.sum(A, axis=1, keepdims=False).shape, \"\\n\")\n",
    "print('axis=0 y keepdims=True')\n",
    "print(np.sum(A, axis=0, keepdims=True))\n",
    "print(np.sum(A, axis=0, keepdims=True).shape, \"\\n\")\n",
    "print('axis=0 y keepdims=False')\n",
    "print(np.sum(A, axis=0, keepdims=False))\n",
    "print(np.sum(A, axis=0, keepdims=False).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-4-1'></a>\n",
    "#### 3.4.1 - Derivadas de las funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las derivadas de las funciones sigmoide y ReLU ya se han vista en la teoría. Si $g^{[l]}()$ es la función de activación, `sigmoide_backward()` y `relu_backward()` están calculando:\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g^{[l]}{'}(Z^{[l]}) \\tag{5}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás para una unica unidad ReLU.\n",
    "\n",
    "    Argumentos:\n",
    "    dA -- gradiente post-activación, de cualquier dimension.\n",
    "    cache -- Es la variable 'Z' que fue almacenada en 'cache' para hacer un calculo de la retropropagacion eficiente.\n",
    "\n",
    "    Devuelve:\n",
    "    dZ -- Gradiente del coste con respecto a Z.\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    # Hacemos una copia profunda de dA con 'copy=True', argumento que crea una copia completa de un objeto y todos sus elementos.\n",
    "    # En el caso de una matriz, esto significa que se crea una nueva matriz con los mismos valores que la matriz original, \n",
    "    # pero en una ubicación de memoria diferente. Esto significa que si cambias un valor en la matriz original, \n",
    "    # no afectará a la copia profunda.\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    # Cuando Z = 0, podemos decidir si queremos que equivalga a 0 o a 1. En este caso, a 0.\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    # Clave asegurar que las dimensiones son correctas\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás para una unica unidad ReLU.\n",
    "\n",
    "    Argumentos:\n",
    "    dA -- gradiente post-activación, de cualquier dimension.\n",
    "    cache -- Es la variable 'Z' que fue almacenada en 'cache' para hacer un calculo de la retropropagacion eficiente.\n",
    "\n",
    "    Devuelve:\n",
    "    dZ -- Gradiente del coste con respecto a Z.\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    # Sigmoide va a ser la ultima capa. dZ es la derivada del coste con respecto a Z de la ultima capa, es decir, dJ/dZ y, \n",
    "    # segun la regla de la cadena, es equivalente a:\n",
    "    #       dZ = dJ/dZ = dJ/dA * dA/dZ, donde dJ/dA = dA, y donde dA/dZ = derivada de la funcion 'sigmoide(Z)', \n",
    "    # y la derivada del sigmoide es: s'(z) = s(z)*(1-s(z))\n",
    "    # Funcion sigmoide\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    # dJ/dZ\n",
    "    dZ = dA * s * (1 - s)\n",
    "    \n",
    "    # Clave asegurar que las dimensiones son correctas\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-4-2'></a>\n",
    "#### 3.4.2 - Cálculo de las derivadas de la función lineal\n",
    "\n",
    "Para la capa $l$, la parte lineal es la ecuación (1) seguida de una activación. Supongamos que ya hemos calculado la derivada $dZ^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial Z^{[l]}}$. A continuación tenemos que hallar $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$. Las tres derivadas parciales se calculan utilizando $dZ^{[l]}$ con las siguientes fórmulas:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1]^T} \\tag{6}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{7}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{J} }{\\partial A^{[l-1]}} = W^{[l]^T} dZ^{[l]} \\tag{8}$$\n",
    "\n",
    "\n",
    "$A^{[l-1]^T}$ es la traspuesta de $A^{[l-1]}$. Implementamos estas tres ecuaciones para obtener la función `backward_lineal()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "137d11e28068848079eb6c315a59f2be",
     "grade": false,
     "grade_id": "cell-418e156a9203fe72",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def backward_lineal(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implementa la parte lineal de la retropropagación para una sola capa (capa l)\n",
    "\n",
    "    Argumentos:\n",
    "    dZ -- Gradiente del coste con respecto a la salida lineal (de la capa l actual).\n",
    "    cache -- tupla de valores (A_prev, W, b) procedentes de forward_lineal() en la capa actual\n",
    "\n",
    "    Devuelve:\n",
    "    dA_prev -- Gradiente del coste con respecto a la activación (de la capa anterior l-1); mismas dimensiones que A_prev\n",
    "    dW -- Gradiente del coste con respecto a W (capa actual l), mismas dimensiones que W\n",
    "    db -- Gradiente del coste con respecto a b (capa actual l), mismas dimensiones que b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T, dZ)    \n",
    "    \n",
    "    # Aseguramos que las dimensiones sean correctas\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)    \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-4-3'></a>\n",
    "#### 3.4.3 - Cálculo de las derivadas de las funciones de activación\n",
    "\n",
    "Por último, se implementa la retropropagación para la capa $l$, el paso *LINEAL->ACTIVACION*. Agrupamos el cálculo lineal con las derivadas de las funciones de activación en una única función. Para usarlas, realizamos las siguientes llamadas:\n",
    "\n",
    "```python\n",
    "dZ = sigmoide_backward(dA, activacion_cache)\n",
    "\n",
    "dZ = relu_backward(dA, activacion_cache)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3497ac4aa36a57278edbfb84a44e1d72",
     "grade": false,
     "grade_id": "cell-6c59263d69168c17",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def backward_activacion_lineal(dA, cache, activacion):\n",
    "    \"\"\"\n",
    "    Implementar la retropropagacion para la capa de la forma LINEAL->ACTIVACION\n",
    "    \n",
    "    Argumentos:\n",
    "    dA -- gradiente post-activación para la capa actual l \n",
    "    cache -- tupla de valores (cache_lineal, activacion_cache) almacenados para calcular la propagación hacia atrás eficientemente\n",
    "    activation -- la funcion de activacion a utilizar en esta capa, almacenada como cadena de texto: \"sigmoide\" o \"relu\".\n",
    "    \n",
    "    Devuelve\n",
    "    dA_prev -- Gradiente del coste con respecto a la activación (de la capa anterior l-1); mismas dimensiones que A_prev\n",
    "    dW -- Gradiente del coste con respecto a W (capa actual l), mismas dimensiones que W\n",
    "    db -- Gradiente del coste con respecto a b (capa actual l), mismas dimensiones que b\n",
    "    \"\"\"\n",
    "    cache_lineal, activacion_cache = cache\n",
    "    \n",
    "    if activacion == \"relu\":\n",
    "        dZ = relu_backward(dA, activacion_cache)\n",
    "        dA_prev, dW, db = backward_lineal(dZ, cache_lineal)\n",
    "    elif activacion == \"sigmoide\":\n",
    "        dZ = sigmoide_backward(dA, activacion_cache)\n",
    "        dA_prev, dW, db = backward_lineal(dZ, cache_lineal)\n",
    "    else:\n",
    "        print(\"¡Error! Solo se admiten relu o sigmoide como parametros en \\\"activacion\\\"\")\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-5'></a>\n",
    "### 3.5 - Actualización de los parámetros\n",
    "\n",
    "Actualizamos los parámetros del modelo aplicando el algoritmo del descenso del gradiente:\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{9}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{10}$$\n",
    "\n",
    "donde $\\alpha$ es la tasa de aprendizaje. \n",
    "\n",
    "Tras calcularlos, se guardarán en un diccionario, como se hizo en el código del archivo de una red neuronal de una capa. Aunque este es el apartado de la red de dos capas, para no crear dos funciones distintas se crea diréctamente la función general de actualización para una red de L capas, es decir, se actualizarán los parámetros $W^{[l]}$ y $b^{[l]}$ para $l = 1, 2, ..., L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a497191ef80b70967006d707cac91c20",
     "grade": false,
     "grade_id": "cell-3cb535f16aba3339",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def actualizar_parametros(parametros_entrada, gradientes, tasa_aprendizaje):\n",
    "    \"\"\"\n",
    "    Actualiza los parámetros utilizando la regla de actualización por descenso de gradiente.\n",
    "    \n",
    "    Argumentos:\n",
    "    parametros_entrada -- diccionario python que contiene los parametros \n",
    "    gradientes -- diccionario python que contiene los gradientes\n",
    "    tasa_aprendizaje -- hiperparametro que representa la tasa de aprendizaje utilizada en la regla de actualizacion\n",
    "    \n",
    "    Devuelve:\n",
    "    parametros -- diccionario python que contiene los parametros actualizados \n",
    "                  parametros[\"W\" + str(l)] = ... \n",
    "                  parametros[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    # Recupera una copia de cada parámetro del diccionario \"parametros_entrada\".\n",
    "    parametros = parametros_entrada.copy()\n",
    "    # numero de capas de la red neuronal (para cada l-esima capa hay un parametro W y otro b)\n",
    "    L = len(parametros) // 2 \n",
    "\n",
    "    # Aplicar regla de actualizacion a cada parametro\n",
    "    for l in range(L):\n",
    "        parametros[\"W\" + str(l + 1)] = parametros[\"W\" + str(l + 1)] - tasa_aprendizaje * gradientes[\"dW\" + str(l + 1)]\n",
    "        parametros[\"b\" + str(l + 1)] = parametros[\"b\" + str(l + 1)] - tasa_aprendizaje * gradientes[\"db\" + str(l + 1)]        \n",
    "\n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Modelo de red neuronal con L capas (L-1 capas ocultas)\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - Inicialización de los parámetros\n",
    "\n",
    "La inicialización de una red neuronal de L capas es más complicada que lo visto hasta ahora, debido a que hay más matrices de pesos y vectores de sesgos. Al implementar la función `inicializacion_profunda()`, es importante asegurarse que las dimensiones sean coherentes entre capas.\n",
    "\n",
    "> Nota: Recuérdese que $n^{[l]}_h$ es el número de unidades (ocultas) en la capa $l$. Por ejemplo, si las dimensiones de la matriz de entrenamiento, $X$, son $(12288, 209)$ (siendo $m=209$ el numero de casos/ejemplos de entrenamiento) entonces:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> <b>Dimensiones de W</b> </td> \n",
    "        <td> <b>Dimensiones de b</b>  </td> \n",
    "        <td> <b>Activacion lineal</b> </td>\n",
    "        <td> <b>Dimensiones de la activación lineal</b> </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> <b>Capa 1</b> </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> <b>Capa 2</b> </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>  \n",
    "   <tr>\n",
    "       <td> <b>Capa L-1</b> </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "   <tr>\n",
    "   <tr>\n",
    "       <td> <b>Capa L</b> </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "**Importante**: Se menciona durante todo el trabajo que, cuando se calcula $W X + b$ en Python, se ejecuta el **_broadcasting_** para vectores y matrices. Por ejemplo, si: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    w_{00}  & w_{01} & w_{02} \\\\\n",
    "    w_{10}  & w_{11} & w_{12} \\\\\n",
    "    w_{20}  & w_{21} & w_{22} \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    x_{00}  & x_{01} & x_{02} \\\\\n",
    "    x_{10}  & x_{11} & x_{12} \\\\\n",
    "    x_{20}  & x_{21} & x_{22} \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    b_0  \\\\\n",
    "    b_1  \\\\\n",
    "    b_2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Entonces $WX + b$ sería:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (w_{00}x_{00} + w_{01}x_{10} + w_{02}x_{20}) + b_0 & (w_{00}x_{01} + w_{01}x_{11} + w_{02}x_{21}) + b_0 & \\cdots \\\\\n",
    "    (w_{10}x_{00} + w_{11}x_{10} + w_{12}x_{20}) + b_1 & (w_{10}x_{01} + w_{11}x_{11} + w_{12}x_{21}) + b_1 & \\cdots \\\\\n",
    "    (w_{20}x_{00} + w_{21}x_{10} + w_{22}x_{20}) + b_2 &  (w_{20}x_{01} + w_{21}x_{11} + w_{22}x_{21}) + b_2 & \\cdots\n",
    "\\end{bmatrix} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideraciones:\n",
    "\n",
    "- La estructura del modelo es *[LINEAL -> RELU] $ \\times$ (L-1) -> LINEAL -> SIGMOIDE*. Es decir, tiene $L-1$ capas utilizando la función de activación ReLU seguidas de una capa de salida con una función de activación sigmoide.\n",
    "- Ya no se utilizará una inicialización aleatoria de la matriz de pesos, para una red de L-capas es insuficiente y además con activaciones ReLU no da buenos resultados. Se utilizará la **inicialización Xavier**, que usa un factor de escala `sqrt(1./layers_dims[l-1])` para los pesos $W^{[l]}$.\n",
    "- Se utilizará una inicialización con ceros para los sesgos: `np.zeros()`. La documentación para [np.zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html).\n",
    "- Se almacenará $n^{[l]}_h$, el número de unidades en las diferentes capas, en una varible `dims_capas`. \n",
    "    - Por ejemplo, esta variable en el archivo anterior donde se clasificaban los datos de un plano sería [2,4,1]: había dos características de entrada, una capa oculta con cuatro unidades ocultas (en la ejecución inicial), y una capa de salida con una unidad. Los parámetros tenían, por tanto, las siguientes dimensiones: `W1` -> (4,2), `b1` -> (4,1), `W2` -> (1,4) y `b2` -> (1,1). Esto es lo que se va a generalizar a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1773f5c69d941998dc8da88f4151e8d3",
     "grade": false,
     "grade_id": "cell-37b22e0664a4949e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def inicializacion_profunda(dims_capas):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    dims_capas -- array python (lista) que contiene las dimensiones de cada capa de nuestra red.\n",
    "    \n",
    "    Devuelve:\n",
    "    parametros -- diccionario python que contiene los parámetros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- matriz de pesos de dimensiones (dims_capas[l], dims_capas[l-1])\n",
    "                    bl -- vector de sesgo de dimensiones (dims_capas[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parametros = {}\n",
    "    # Numero de capas en la red neuronal\n",
    "    L = len(dims_capas)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        # Multiplicar por 0.01 ya no es suficiente con un modelo mas grande. Utilizamos la inicializacion Xavier\n",
    "        parametros['W' + str(l)] = np.random.randn(dims_capas[l], dims_capas[l - 1])  / np.sqrt(dims_capas[l-1]) #*0.01\n",
    "        parametros['b' + str(l)] = np.zeros((dims_capas[l], 1))        \n",
    "        \n",
    "        # Asegurar que las dimensiones son correctas\n",
    "        assert(parametros['W' + str(l)].shape == (dims_capas[l], dims_capas[l - 1]))\n",
    "        assert(parametros['b' + str(l)].shape == (dims_capas[l], 1))\n",
    "        \n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Propagación hacia delante (_forward propagation_)\n",
    "\n",
    "Combinamos las dos funciones que se utilizaron en la red de dos capas en una tercera (tercer paso) con esta estructura:\n",
    "\n",
    "- [LINEAL -> RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDE\n",
    "\n",
    "Esta función con validez general llama a la función `forward_activacion_lineal(activacion=\"relu\")` $L-1$ veces y por último llama a la misma función pero con activación sigmoide: `forward_activacion_lineal(activacion=\"sigmoide\")`.\n",
    "\n",
    "<img src=\"imagenes/propagacion_hacia_delante.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> <b>Figura 3</b> : Modelo [LINEAL -> RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDE</center></caption><br>\n",
    "\n",
    "> Nota: En el codigo de la función, la variable `AL` particulariza la ecuación (2) para la activación sigmoide en la capa L, la última, e indica $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. Y, como se ha señalado constantemente en el trabajo, la salida de la activación de la útlima capa equivale a $\\hat{Y}$, es decir, la predicción final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0071c19f83d4b851dc8a67e66545262",
     "grade": false,
     "grade_id": "cell-9a8ec52ec8f6e04a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def propagacion_L_capas(X, parametros):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia delante para el cálculo [LINEAL->RELU]*(L-1)->LINEAL->SIGMOIDE\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- datos, array numpy de dimensiones (caracteristicas por ejemplo, número de ejemplos)\n",
    "    parametros -- salida de inicializacion_profunda()\n",
    "    \n",
    "    Devuelve:\n",
    "    AL -- valor de activación de la capa de salida\n",
    "    caches -- lista de caches que contienen cada cache de forward_activacion_lineal() (hay L, indexadas de 0 a L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    # La activacion inicial, A[0], es la capa de entrada\n",
    "    A = X\n",
    "    # numero de capas de la red neuronal (para cada l-esima capa hay un parametro W y otro b)\n",
    "    L = len(parametros) // 2\n",
    "    \n",
    "    # Implementar [LINEAL -> RELU]*(L-1). Añadimos \"cache\" a la lista de \"caches\".\n",
    "    # El bucle comienza en 1 porque la capa 0 es la de entrada\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = forward_activacion_lineal(A_prev, \n",
    "                                             parametros['W' + str(l)], \n",
    "                                             parametros['b' + str(l)], \n",
    "                                             activacion='relu')\n",
    "        caches.append(cache)        \n",
    "    \n",
    "    # Implementar LINEAL -> SIGMOIDE. Añadimos \"cache\" a la lista de \"caches\".\n",
    "    AL, cache = forward_activacion_lineal(A, \n",
    "                                          parametros['W' + str(L)], \n",
    "                                          parametros['b' + str(L)], \n",
    "                                          activacion='sigmoide')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    # Nos aseguramos que las dimensiones sean correctas (n_y, m)\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto se habría implementado un proceso de propagación hacia delante completo que toma la matriz de entrada, $X$, y devuelve un vector $A^{[L]}$ con las predicciones. También lleva un registro de todos los valores intermedios calculados en \"caches\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Cálculo del coste global\n",
    "\n",
    "Igual que en el apartado [3.3 - Cálculo del coste global](#3-3), utilizando $A^{[L]}$ se puede calcular el coste de las predicciones de la red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Retropropagación (_backward propagation_)\n",
    "\n",
    "A la hora de implementar la retropropagación, utilizaremos la misma lógica que para el modelo de dos capas, donde se aprovecha la \"cache\" almacenada. En cada iteración de la función `propagacion_L_capas()` se guarda un cache que contiene la tupla de valores (Z, A, W, b). En la retropropagación, se utilizan esas variables para calcular los gradientes. Por lo tanto, en la función que se crea a continuación, `retropropagacion_L_capas()`, se itera sobre toda las capas ocultas, empezando por la capa final, $L$, hasta la capa $1$. En cada paso, se utiliza el \"cache\" correspondiente a la capa $l$ para realizar la retropropagación (las derivadas) sobre la capa $l$. Este procedimiento se esquematiza en la figura 4.\n",
    "\n",
    "<img src=\"imagenes/retropropagacion.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center><b>Figura 4</b>: Retropropagación completa en un modelo de L capas.</center></caption>\n",
    "\n",
    "Para **inicializar la retropropagación** empezando por la última capa, sabemos que la activación es una función sigmoide, con salida $A^{[L]} = \\sigma(Z^{[L]})$. Por lo tanto el primer paso es calcular `dAL` $= \\frac{\\partial \\mathcal{J}}{\\partial A^{[L]}}$. Si la salida es sigmoide el valor de esta derivada, vectorizado a todos los ejemplos de la red, es:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{J}}{\\partial A^{[L]}} = -\\left(\\frac{Y}{A^{[L]}} - \\frac{1 - Y}{1- A^{[L]}}\\right)\\tag{11} $$\n",
    "\n",
    "En python se puede aplicar de la siguiente manera:\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivada del coste con respecto a AL\n",
    "```\n",
    "\n",
    "A continuación se utiliza este gradiente de post-activación `dAL` para continuar hacar atrás en las capas. Como se ve en la figura 3, lo siguiente sería pasar del sigmoide a la derivada de la función lineal y, para ello, nos valdremos de la función `sigmoide_backward()`. El \"cache\" que utilizará esta función será el correspondiente, almacenado durante la ejecución de `propagacion_L_capas()`. Con esto se termina el bloque la combinación [LINEAL -> SIGMOIDE_backward]\n",
    "\n",
    "Tras esto, se utilizará un bucle `for` para iterar la retropropagación en las otras capas utilizando la combinación la combinación [LINEAL -> RELU_backward]. Los valores para cada capa de `dA`, `dW`, y `db` se almacenarán en un diccionario utilizando la siguiente fórmula:\n",
    "\n",
    "$$gradientes[\"dW\" + str(l)] = dW^{[l]}\\tag{12} $$\n",
    "\n",
    "Por ejemplo, para $l=4$ se guardaría $dW^{[4]}$ en `gradientes[\"dW4\"]`.\n",
    "\n",
    "En definitiva, se van a combinar las dos funciones que se utilizaron en la red de dos capas en una tercera (tercer paso) con esta estructura:\n",
    "\n",
    "LINEAL -> SIGMOIDE backward -> [LINEAL -> RELU backward] $\\times$ (L-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3e23a2b5f3b33e264a122b3c4b0d760",
     "grade": false,
     "grade_id": "cell-9eec96b6d83ff809",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def retropropagacion_L_capas(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implementa la retropropagacion para el grupo [LINEAL->RELU] * (L-1) -> LINEAL -> SIGMOIDE\n",
    "    \n",
    "    Argumentos:\n",
    "    AL -- vector de predicciones, salida de la propagación hacia delante (propagacion_L_capas())\n",
    "    Y -- Vector de etiquetas verdaderas de dimensiones (1, número de casos de entrenamiento).\n",
    "    caches -- lista del cache de cada capa. Contiene:\n",
    "                cada cache linear_activation_forward(\"relu\") (es caches[l], para l en range(L-1), es decir, l = 0...L-2)\n",
    "                el cache de linear_activation_forward(\"sigmoide\") (es caches[L-1])\n",
    "    \n",
    "    Devuelve:\n",
    "    gradientes -- Un diccionario con los gradientes\n",
    "                  gradientes[\"dA\" + str(l)] \n",
    "                  gradientes[\"dW\" + str(l)]\n",
    "                  gradientes[\"db\" + str(l)]\n",
    "    \"\"\"\n",
    "    gradientes = {}\n",
    "    # Numero de capas\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    # Tras esta linea, Y tiene las mismas dimensiones que AL.\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Inicializar la retropropagacion\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Capa L: gradientes de (SIGMOID -> LINEAL). Entrada: \"dAL, cache_actual\". \n",
    "    #                                            Salidas: \"gradientes[\"dAL-1\"], gradientes[\"dWL\"], gradientes[\"dbL\"]\n",
    "    # la capa L es el ultimo valor de la lista \"caches\", que contiene L elementos, y al empezar el indice por 0, el ultimo es L-1\n",
    "    cache_actual = caches[L-1] # tambien cache_actual = caches[-1] nos lleva al ultimo elemento\n",
    "    dA_prev_temp, dW_temp, db_temp = backward_activacion_lineal(dAL, cache_actual, activacion = \"sigmoide\")\n",
    "    gradientes[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    gradientes[\"dW\" + str(L)] = dW_temp\n",
    "    gradientes[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    # Bucle de l=L-2 a l=0 para implementar los gradientes de (RELU -> LINEAL). range(L-1) va de 0 a L-2\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Entradas: \"gradientes[\"dA\" + str(l + 1)], cache_actual\". \n",
    "        # Salidas: \"gradientes[\"dA\" + str(l)] , gradientes[\"dW\" + str(l + 1)] , gradientes[\"db\" + str(l + 1)]\n",
    "        # La suma (l + 1) es por el hecho de que nuestro indice va de (0, L-1) y no de (1, L), por lo que la equivalencia  \n",
    "        # entre el bucle y las capas se logra sumando 1.\n",
    "        cache_actual = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_activacion_lineal(gradientes[\"dA\" + str(l + 1)], \n",
    "                                                                    cache_actual, \n",
    "                                                                    activacion = \"relu\")\n",
    "        # Otra forma equivalente, que es lo que tenemos encapsulado en 2 lineas en la funcion backward_activacion_lineal():\n",
    "        # dA_prev_temp, dW_temp, db_temp = backward_lineal(relu_backward(grads[\"dA\" + str(l + 1)], \n",
    "        #                                                  current_cache[1]), current_cache[0])\n",
    "        gradientes[\"dA\" + str(l)] = dA_prev_temp\n",
    "        gradientes[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradientes[\"db\" + str(l + 1)] = db_temp        \n",
    "\n",
    "    return gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-5'></a>\n",
    "### 4.5 - Actualización de los parámetros\n",
    "\n",
    "Igual que en el apartado [3.5 - Actualización de los parámetros](#3-5)."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
